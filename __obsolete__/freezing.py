roberta_unfreeze_list=[
'embeddings.word_embeddings.weight',
'embeddings.position_embeddings.weight',
'embeddings.token_type_embeddings.weight',
'embeddings.LayerNorm.weight',
'embeddings.LayerNorm.bias',
'encoder.layer.9.attention.self.query.weight',
'encoder.layer.9.attention.self.query.bias',
'encoder.layer.9.attention.self.key.weight',
'encoder.layer.9.attention.self.key.bias',
'encoder.layer.9.attention.self.value.weight',
'encoder.layer.9.attention.self.value.bias',
'encoder.layer.9.attention.output.dense.weight',
'encoder.layer.9.attention.output.dense.bias',
'encoder.layer.9.attention.output.LayerNorm.weight',
'encoder.layer.9.attention.output.LayerNorm.bias',
'encoder.layer.9.intermediate.dense.weight',
'encoder.layer.9.intermediate.dense.bias',
'encoder.layer.9.output.dense.weight',
'encoder.layer.9.output.dense.bias',
'encoder.layer.9.output.LayerNorm.weight',
'encoder.layer.9.output.LayerNorm.bias',
'encoder.layer.10.attention.self.query.weight',
'encoder.layer.10.attention.self.query.bias',
'encoder.layer.10.attention.self.key.weight',
'encoder.layer.10.attention.self.key.bias',
'encoder.layer.10.attention.self.value.weight',
'encoder.layer.10.attention.self.value.bias',
'encoder.layer.10.attention.output.dense.weight',
'encoder.layer.10.attention.output.dense.bias',
'encoder.layer.10.attention.output.LayerNorm.weight',
'encoder.layer.10.attention.output.LayerNorm.bias',
'encoder.layer.10.intermediate.dense.weight',
'encoder.layer.10.intermediate.dense.bias',
'encoder.layer.10.output.dense.weight',
'encoder.layer.10.output.dense.bias',
'encoder.layer.10.output.LayerNorm.weight',
'encoder.layer.10.output.LayerNorm.bias',
'encoder.layer.11.attention.self.query.weight',
'encoder.layer.11.attention.self.query.bias',
'encoder.layer.11.attention.self.key.weight',
'encoder.layer.11.attention.self.key.bias',
'encoder.layer.11.attention.self.value.weight',
'encoder.layer.11.attention.self.value.bias',
'encoder.layer.11.attention.output.dense.weight',
'encoder.layer.11.attention.output.dense.bias',
'encoder.layer.11.attention.output.LayerNorm.weight',
'encoder.layer.11.attention.output.LayerNorm.bias',
'encoder.layer.11.intermediate.dense.weight',
'encoder.layer.11.intermediate.dense.bias',
'encoder.layer.11.output.dense.weight',
'encoder.layer.11.output.dense.bias',
'encoder.layer.11.output.LayerNorm.weight',
'encoder.layer.11.output.LayerNorm.bias',
'pooler.dense.weight',
'pooler.dense.bias',
]



distilbert_unfreeze_list = [
'embeddings.word_embeddings.weight',
'embeddings.position_embeddings.weight',
'embeddings.LayerNorm.weight',
'embeddings.LayerNorm.bias', 
'transformer.layer.5.attention.q_lin.weight',
'transformer.layer.5.attention.q_lin.bias',
'transformer.layer.5.attention.k_lin.weight',
'transformer.layer.5.attention.k_lin.bias',
'transformer.layer.5.attention.v_lin.weight',
'transformer.layer.5.attention.v_lin.bias',
'transformer.layer.5.attention.out_lin.weight',
'transformer.layer.5.attention.out_lin.bias',
'transformer.layer.5.sa_layer_norm.weight',
'transformer.layer.5.sa_layer_norm.bias',
'transformer.layer.5.ffn.lin1.weight',
'transformer.layer.5.ffn.lin1.bias',
'transformer.layer.5.ffn.lin2.weight',
'transformer.layer.5.ffn.lin2.bias',
'transformer.layer.5.output_layer_norm.weight',
'transformer.layer.5.output_layer_norm.bias',
]

"""
embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.LayerNorm.weight
embeddings.LayerNorm.bias
transformer.layer.0.attention.q_lin.weight
transformer.layer.0.attention.q_lin.bias
transformer.layer.0.attention.k_lin.weight
transformer.layer.0.attention.k_lin.bias
transformer.layer.0.attention.v_lin.weight
transformer.layer.0.attention.v_lin.bias
transformer.layer.0.attention.out_lin.weight
transformer.layer.0.attention.out_lin.bias
transformer.layer.0.sa_layer_norm.weight
transformer.layer.0.sa_layer_norm.bias
transformer.layer.0.ffn.lin1.weight
transformer.layer.0.ffn.lin1.bias
transformer.layer.0.ffn.lin2.weight
transformer.layer.0.ffn.lin2.bias
transformer.layer.0.output_layer_norm.weight
transformer.layer.0.output_layer_norm.bias
transformer.layer.1.attention.q_lin.weight
transformer.layer.1.attention.q_lin.bias
transformer.layer.1.attention.k_lin.weight
transformer.layer.1.attention.k_lin.bias
transformer.layer.1.attention.v_lin.weight
transformer.layer.1.attention.v_lin.bias
transformer.layer.1.attention.out_lin.weight
transformer.layer.1.attention.out_lin.bias
transformer.layer.1.sa_layer_norm.weight
transformer.layer.1.sa_layer_norm.bias
transformer.layer.1.ffn.lin1.weight
transformer.layer.1.ffn.lin1.bias
transformer.layer.1.ffn.lin2.weight
transformer.layer.1.ffn.lin2.bias
transformer.layer.1.output_layer_norm.weight
transformer.layer.1.output_layer_norm.bias
transformer.layer.2.attention.q_lin.weight
transformer.layer.2.attention.q_lin.bias
transformer.layer.2.attention.k_lin.weight
transformer.layer.2.attention.k_lin.bias
transformer.layer.2.attention.v_lin.weight
transformer.layer.2.attention.v_lin.bias
transformer.layer.2.attention.out_lin.weight
transformer.layer.2.attention.out_lin.bias
transformer.layer.2.sa_layer_norm.weight
transformer.layer.2.sa_layer_norm.bias
transformer.layer.2.ffn.lin1.weight
transformer.layer.2.ffn.lin1.bias
transformer.layer.2.ffn.lin2.weight
transformer.layer.2.ffn.lin2.bias
transformer.layer.2.output_layer_norm.weight
transformer.layer.2.output_layer_norm.bias
transformer.layer.3.attention.q_lin.weight
transformer.layer.3.attention.q_lin.bias
transformer.layer.3.attention.k_lin.weight
transformer.layer.3.attention.k_lin.bias
transformer.layer.3.attention.v_lin.weight
transformer.layer.3.attention.v_lin.bias
transformer.layer.3.attention.out_lin.weight
transformer.layer.3.attention.out_lin.bias
transformer.layer.3.sa_layer_norm.weight
transformer.layer.3.sa_layer_norm.bias
transformer.layer.3.ffn.lin1.weight
transformer.layer.3.ffn.lin1.bias
transformer.layer.3.ffn.lin2.weight
transformer.layer.3.ffn.lin2.bias
transformer.layer.3.output_layer_norm.weight
transformer.layer.3.output_layer_norm.bias
transformer.layer.4.attention.q_lin.weight
transformer.layer.4.attention.q_lin.bias
transformer.layer.4.attention.k_lin.weight
transformer.layer.4.attention.k_lin.bias
transformer.layer.4.attention.v_lin.weight
transformer.layer.4.attention.v_lin.bias
transformer.layer.4.attention.out_lin.weight
transformer.layer.4.attention.out_lin.bias
transformer.layer.4.sa_layer_norm.weight
transformer.layer.4.sa_layer_norm.bias
transformer.layer.4.ffn.lin1.weight
transformer.layer.4.ffn.lin1.bias
transformer.layer.4.ffn.lin2.weight
transformer.layer.4.ffn.lin2.bias
transformer.layer.4.output_layer_norm.weight
transformer.layer.4.output_layer_norm.bias
transformer.layer.5.attention.q_lin.weight
transformer.layer.5.attention.q_lin.bias
transformer.layer.5.attention.k_lin.weight
transformer.layer.5.attention.k_lin.bias
transformer.layer.5.attention.v_lin.weight
transformer.layer.5.attention.v_lin.bias
transformer.layer.5.attention.out_lin.weight
transformer.layer.5.attention.out_lin.bias
transformer.layer.5.sa_layer_norm.weight
transformer.layer.5.sa_layer_norm.bias
transformer.layer.5.ffn.lin1.weight
transformer.layer.5.ffn.lin1.bias
transformer.layer.5.ffn.lin2.weight
transformer.layer.5.ffn.lin2.bias
transformer.layer.5.output_layer_norm.weight
transformer.layer.5.output_layer_norm.bias
"""
