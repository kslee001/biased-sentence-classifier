{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook includes Code for:\n",
    "## Whole Texts/single sentences. needs to be adapted to tweets\n",
    "##### (also includes code for tokenizing text and get sentences,  finding keyword in sentences)\n",
    "#### - Extracting Keywords:\n",
    "- own Approach, \n",
    "- KeyBERT\n",
    "\n",
    "#### - Generate Synonyms: \n",
    "- Sense2Vec: takes prepared sentence as input\n",
    "- ConceptNet: only needs one word. Download vector files!\n",
    "- Wordnet: only needs one world. Not sure if it works with spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "AzsYNBGiAnyu",
    "outputId": "a62d23f0-0b60-43b8-d634-ac8ec70cc01b"
   },
   "outputs": [],
   "source": [
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import requests\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "from keybert import KeyBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Text and define Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"\"\n",
    "#f = open(PATH + text,\"r\")\n",
    "# f = open(text,\"r\")\n",
    "# full_text = f.read() \n",
    "\n",
    "full_text= \"One of the oldest breeds in the world, the Siamese were introduced to Europe through England as the Royal Cat of Siam (now Thailand). Siamese cats are endlessly curious, and require a lot of attention, so they are good for people who can afford to spend time playing with them. They always seek new hideaways in and around the house, and they even seem to be enjoying watching television. As a breed, Siamese cats are great at learning tricks. They are also eager to show their owners how to do exactly what they want them to do. Being incredibly pretty, the Siamese are frequently portrayed in movies, including some of the most famous animated ones like 'The Aristocats'.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"EN\"\n",
    "#language = \"DE\"\n",
    "number_of_generated_keywords = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_KW = False\n",
    "keywords = []\n",
    "#print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLgXL1RcAnyx"
   },
   "source": [
    "## Keyword Extraction\n",
    "Get important keywords from the text and filter those keywords that are present in the text.\n",
    "- own Approach\n",
    "- KeyBERT\n",
    "- NLTK could also be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokeinzes Texts and gives Part Of Speech\n",
    "# import nltk\n",
    "# from nltk import word_tokenize,pos_tag\n",
    "# tokens = word_tokenize(full_text)\n",
    "# tag=pos_tag(tokens)\n",
    "# print(tag)\n",
    "# ne_tree = nltk.ne_chunk(tag)\n",
    "# print(ne_tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please note that this is for whole texts. needs to be adapted to single sentences/ a df with tweets\n",
    "\n",
    "def getKeywords_own(text, language, own_KW, keywords, number_of_generated_keywords): \n",
    "    \"\"\"\n",
    "     Der Keyword extractor extrahiert zwar die Keywords, schreibt sie aber leider automatisch klein. \n",
    "     Lösung: Der Originaltext wird klein geschrieben, und die Positionen der klein geschriebenen Keywords ausfindig gemacht \"start_index & end_index\".\n",
    "     Danach wird die Position im Originaltext gesucht und das richtige Keywort mit der richtigen Groß & Kleinschreibung extrahiert und in der Liste  \"out\" gespeichert.\n",
    "     \n",
    "     The keyword extractor extracts the keywords, but unfortunately automatically writes them in lowercase. \n",
    "     Solution: The original text is lowercased, and the positions of the lowercased keywords are found \"start_index & end_index\".\n",
    "     Then the position in the original text is searched and the correct keyword with the correct upper & lower case is extracted and stored in the \"out\" list.\n",
    "    \"\"\"\n",
    "\n",
    "    out = []\n",
    "    keywords_lower = []\n",
    "    \n",
    "#     #german \n",
    "#     if language ==\"DE\":\n",
    "#         extractor = pke.unsupervised.MultipartiteRank() #Keyword Extractor model #!!! Problem: lowercases every word    \n",
    "#         extractor.load_document(input = text, language = 'de')\n",
    "#         #pos = {'NOUN'} # just use nouns as keywords\n",
    "#         pos = {'VERB', 'ADJ', 'NOUN','PROPN'}\n",
    "#         stoplist = list(string.punctuation)\n",
    "#         stoplist += stopwords.words('german') \n",
    "        \n",
    "        \n",
    "    #spanish \n",
    "    if language ==\"ES\":\n",
    "        extractor = pke.unsupervised.MultipartiteRank() #Keyword Extractor model #!!! Problem: lowercases every word    \n",
    "        extractor.load_document(input = text, language = 'es')\n",
    "        #pos = {'NOUN'} # just use nouns as keywords\n",
    "        pos = {'VERB', 'ADJ', 'NOUN','PROPN'}\n",
    "        stoplist = list(string.punctuation)\n",
    "        stoplist += stopwords.words('spanish') \n",
    "\n",
    "    #english\n",
    "    elif language == \"EN\":\n",
    "        extractor = pke.unsupervised.MultipartiteRank()\n",
    "        extractor.load_document(input = text, language = 'en')\n",
    "        #pos = {'NOUN'} # just use nouns as keyword\n",
    "        pos = {'VERB', 'ADJ', 'NOUN','PROPN'}\n",
    "        stoplist = list(string.punctuation)\n",
    "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-'] #not sure about that\n",
    "        stoplist += stopwords.words('english')\n",
    "\n",
    "\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "    #    threshold/method parameters.\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                  threshold=0.75,\n",
    "                                  method='average')\n",
    "        \n",
    "    keys = extractor.get_n_best(number_of_generated_keywords) # get keywords from text, these keywords are automatically lowercased!\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Ab hier wird sich um das Groß/klein Problem gekümmert.\n",
    "    From here on, the problem of capital/lowercase is taken care of.\n",
    "    \"\"\"\n",
    "    for key in keys:\n",
    "        keywords_lower.append(key[0]) #create a list with just the lowercased keys \n",
    "    \n",
    "    if own_KW == True:\n",
    "        sentences = tokenize_sentences(text,nlp) # Make multiple sentences of the text\n",
    "        for sentence in sentences:\n",
    "            for key in keys:\n",
    "                sentence_lower = sentence.lower()\n",
    "                start_index = sentence_lower.find(key[0]) #seach for the keyword in the lowered text    \n",
    "\n",
    "                if start_index is not -1: #give me just the sentences with include the keyword\n",
    "                    end_index = start_index + len(key[0]) # if the start_index is not -1 / get the end position of the keyword\n",
    "                    real_KW = sentence[start_index : end_index]\n",
    "\n",
    "                    if own_KW == True:\n",
    "                        if real_KW not in out:\n",
    "                            out.append(real_KW)\n",
    "\n",
    "                    elif own_KW == False:\n",
    "                        if real_KW not in out:\n",
    "                            out.append(real_KW[0])\n",
    "    \n",
    "    elif own_KW == False:\n",
    "        sentence = text\n",
    "        for key in keys:\n",
    "            sentence_lower = sentence.lower()\n",
    "            start_index = sentence_lower.find(key[0])\n",
    "            if start_index is not -1: #give me just the sentences with include the keyword\n",
    "                end_index = start_index + len(key[0])\n",
    "                real_KW = sentence[start_index : end_index]\n",
    "                if own_KW == True:\n",
    "                    if real_KW not in out:\n",
    "                        out.append(real_KW)\n",
    "\n",
    "                elif own_KW == False:\n",
    "                    if real_KW not in out:\n",
    "                        out.append(real_KW)    \n",
    "        \n",
    "    return out#, print(out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords_own = getKeywords_own(full_text, language, own_KW, keywords, number_of_generated_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siamese', 'oldest breeds', 'Siamese cats', 'introduced', 'Europe', 'world', 'England', 'Royal Cat', 'movies', 'Siam', 'portrayed', 'lot', 'require', 'Thailand', 'attention', 'curious', 'including', 'show', 'great', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(Keywords_own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cats', 0.2469), ('cat', 0.2033), ('siamese', 0.1528), ('thailand', 0.1012), ('aristocats', 0.067)]\n",
      "[('siamese cats are great', 0.6959), ('siamese cats are endlessly', 0.6518), ('as breed siamese cats', 0.5754), ('breed siamese cats are', 0.5726), ('cats are endlessly curious', 0.5715)]\n"
     ]
    }
   ],
   "source": [
    "def getKeywords_KeyBERT(text):\n",
    "    # script.py\n",
    "    keyword_list = []\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(text)\n",
    "\n",
    "    print(kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None)) # single word\n",
    "    print(kw_model.extract_keywords(text, keyphrase_ngram_range=(4, 4), stop_words=None)) # word spans\n",
    "    \n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "    \n",
    "    for keyword in keywords: \n",
    "        keyword = keyword[0]\n",
    "        keyword_list.append(keyword)\n",
    "    return keyword_list\n",
    "\n",
    "Keywords_KeyBERT = getKeywords_KeyBERT(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'cat', 'siamese', 'thailand', 'aristocats']\n"
     ]
    }
   ],
   "source": [
    "print(Keywords_KeyBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAl-ief1Anyy"
   },
   "source": [
    "## Sentence Mapping\n",
    "For each keyword get the sentences from text containing that keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "0Ytt673tAnyy",
    "outputId": "8022c77f-e16b-4097-c175-fbc9a80599c0"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 10 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 10]\n",
    "    return sentences\n",
    "\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        #word = word.lower()\n",
    "        word = word.strip()\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "\n",
    "    delete_keys = []\n",
    "    for k in keyword_sentences.keys():\n",
    "        if len(keyword_sentences[k]) == 0:\n",
    "            delete_keys.append(k)\n",
    "    for del_key in delete_keys:\n",
    "        del keyword_sentences[del_key]\n",
    "    return keyword_sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenize_sentences(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = Keywords_own\n",
    "keywords = Keywords_KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': ['Siamese cats are endlessly curious, and require a lot of attention, so they are good for people who can afford to spend time playing with them.',\n",
       "  'As a breed, Siamese cats are great at learning tricks.'],\n",
       " 'cat': ['One of the oldest breeds in the world, the Siamese were introduced to Europe through England as the Royal Cat of Siam (now Thailand).'],\n",
       " 'siamese': ['Siamese cats are endlessly curious, and require a lot of attention, so they are good for people who can afford to spend time playing with them.',\n",
       "  \"Being incredibly pretty, the Siamese are frequently portrayed in movies, including some of the most famous animated ones like 'The Aristocats'.\",\n",
       "  'One of the oldest breeds in the world, the Siamese were introduced to Europe through England as the Royal Cat of Siam (now Thailand).',\n",
       "  'As a breed, Siamese cats are great at learning tricks.'],\n",
       " 'thailand': ['One of the oldest breeds in the world, the Siamese were introduced to Europe through England as the Royal Cat of Siam (now Thailand).'],\n",
       " 'aristocats': [\"Being incredibly pretty, the Siamese are frequently portrayed in movies, including some of the most famous animated ones like 'The Aristocats'.\"]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_sentence_mapping = get_sentences_for_keyword(keywords, sentences)\n",
    "keyword_sentence_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used in this notebook\n",
    "\n",
    "# def KW_position(keywords, texty):\n",
    "#     sentences = tokenize_sentences(text, nlp)  \n",
    "#     keyword_sentence_mapping = get_sentences_for_keyword(keywords,sentences)\n",
    "#     for sent in keyword_sentence_mapping.items():\n",
    "#         text = sent.lower()\n",
    "        \n",
    "#         for keyword in keywords:\n",
    "#             start_index = text.find(keyword)\n",
    "#             end_index = start_index + len(keyword) # if the start_index is not -1\n",
    "    \n",
    "#     return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2IwHlwhAnyz"
   },
   "source": [
    "## Additional Distractor generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FÜR DEUTSCHES WORDNET (GERMANET)\n",
    "#XML FILE NÖTIG, MATTHIAS SCHREIBEN!!\n",
    "#https://uni-tuebingen.de/en/142828\n",
    "\n",
    "# from germanetpy import germanet\n",
    "# germanet_object = germanet.Germanet(\"/home/projectdrive/Dropbox (FH St. Pölten)/Quickspeech/MCQ_FH/datadir/\")\n",
    "# !pip install pip install germanetpy\n",
    "\n",
    "# from germanetpy.germanet import Germanet\n",
    "# from germanetpy.frames import Frames\n",
    "# from germanetpy.filterconfig import Filterconfig\n",
    "# from germanetpy.synset import WordCategory, WordClass\n",
    "# germanet = Germanet(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Sense2Vec \n",
    "- Takes whole input sentence and vectorizes it.\n",
    "- you need to download language specific vectors from web.\n",
    "example: https://github.com/explosion/sense2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "normalized_levenshtein = NormalizedLevenshtein()\n",
    "\n",
    "def filter_same_sense_words(original,wordlist):\n",
    "    filtered_words=[]\n",
    "    base_sense =original.split('|')[1] \n",
    "    for eachword in wordlist:\n",
    "        if eachword[0].split('|')[1] == base_sense:\n",
    "            filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
    "    return filtered_words\n",
    "\n",
    "def get_highest_similarity_score(wordlist,wrd):\n",
    "    score=[]\n",
    "    for each in wordlist:\n",
    "        score.append(normalized_levenshtein.similarity(each.lower(),wrd.lower()))\n",
    "    return max(score)\n",
    "\n",
    "def sense2vec_get_words(word,s2v,topn,question):\n",
    "    output = []\n",
    "    print (\"Keyword \",word)\n",
    "    try:\n",
    "        sense = s2v.get_best_sense(word, senses= [\"NOUN\", \"PERSON\",\"PRODUCT\",\"LOC\",\"ORG\",\"EVENT\",\"NORP\",\"WORK OF ART\",\"FAC\",\"GPE\",\"NUM\",\"FACILITY\"])\n",
    "        most_similar = s2v.most_similar(sense, n=topn)\n",
    "        # print (most_similar)\n",
    "        output = filter_same_sense_words(sense,most_similar)\n",
    "        print (\"Distractors_sense2vec \",output)\n",
    "    except:\n",
    "        output =[]\n",
    "\n",
    "    threshold = 0.6\n",
    "    final=[word]\n",
    "    \n",
    "    for word in question:   \n",
    "        checklist =question.split()\n",
    "    for x in output:\n",
    "        if get_highest_similarity_score(final,x)<threshold and x not in final and x not in checklist:\n",
    "            final.append(x)\n",
    "    \n",
    "    return final[1:]\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphrase\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "DG_Ygdz9Anyz",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# needed for Wordnet and Conceptnet!\n",
    "\n",
    "def get_wordsense(sent,word):\n",
    "   # word= word.lower()\n",
    "    print(sent)\n",
    "    print(word)\n",
    "    \n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")   \n",
    "        \n",
    "    synsets = wn.synsets(word,'n') \n",
    "    \n",
    "    print(synsets)\n",
    "#     if synsets:\n",
    "#         wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "#         adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "#         lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "#         return synsets[lowest_index]\n",
    "#     else:\n",
    "    return #None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractors_wordnet(word):\n",
    "    distractors_wn=[]\n",
    "    try:\n",
    "        syn = wn.synsets(word,'n')[0] #en\n",
    "        #syn = gn.synsets(word)#de\n",
    "        #print(syn)\n",
    "        word= word.lower()\n",
    "        orig_word = word\n",
    "        if len(word.split())>0:\n",
    "            word = word.replace(\" \",\"_\")\n",
    "        hypernym = syn.hypernyms()\n",
    "        if len(hypernym) == 0: \n",
    "            return distractors\n",
    "        for item in hypernym[0].hyponyms():\n",
    "            name = item.lemmas()[0].name()\n",
    "            print (\"Synonym\",name)\n",
    "            if name == orig_word:\n",
    "                continue\n",
    "            name = name.replace(\"_\",\" \")\n",
    "            name = \" \".join(w.capitalize() for w in name.split())\n",
    "            if name is not None and name not in distractors_wn:\n",
    "                distractors_wn.append(name)\n",
    "    except:\n",
    "        print (\"Wordnet distractors not found\")\n",
    "    return distractors_wn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CONCEPTNET_______________________________________\n",
    "\n",
    "def get_distractors_conceptnet(word): #!!! IMPORTANT - CHOOSE LANGUAGE!!\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list_cn = [] \n",
    "#     #url = \"http://api.conceptnet.io/query?filter=/c/de/node=/c/de/%s/n&rel=/r/PartOf&start=/c/de/%s&limit=5\"%(word,word)\n",
    "#     url = \"http://api.conceptnet.io/query?node=/c/de/%s/n&rel=/r/IsA&start=/c/de/%s&limit=5\"%(word,word)\n",
    "#     #url = \"http://api.conceptnet.io/query?node=/c/de/%s/n&rel=/r/TypeOf&start=/c/de/%s&limit=5\"%(word,word)\n",
    "#     #url = \"http://api.conceptnet.io/query?node=/c/de/%s/n&rel=/r/RelatedTo&start=/c/de/%s&limit=5\"%(word,word)\n",
    "\n",
    "    if language == \"EN\":\n",
    "        url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/IsA&start=/c/en/%s&limit=5\"%(word,word) #ENGLISH\n",
    "        \n",
    "    if language == \"ES\":\n",
    "        url = \"http://api.conceptnet.io/query?node=/c/es/%s/n&rel=/r/IsA&start=/c/es/%s&limit=5\"%(word,word) #SPANISH\n",
    "    \n",
    "    obj = requests.get(url).json()\n",
    "   \n",
    "    #print(obj)\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        #url2 = \"http://api.conceptnet.io/query?filter=/c/de/node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/IsA&end=%s&limit=10\"%(link,link)\n",
    "        #url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/TypeOf&end=%s&limit=10\"%(link,link)\n",
    "        #url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/RelatedTo&end=%s&limit=10\"%(link,link)\n",
    "        \n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "           \n",
    "            if word2 not in distractor_list_cn and original_word.capitalize() not in word2.capitalize():\n",
    "                distractor_list_cn.append(word2)                 \n",
    "    return distractor_list_cn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#__________________________________________________________________________________________________\n",
    "def get_distractors_s2v (word,question,sense2vecmodel,sentencemodel,top_n,lambdaval):\n",
    "    distractor_list_s2v = {}\n",
    "    distractors_s2v = sense2vec_get_words(word,sense2vecmodel,top_n,question)\n",
    "    distractors_new = [word.capitalize()]\n",
    "    distractors_new.extend(distractors_s2v)\n",
    "    # print (\"distractors_new .. \",distractors_new)\n",
    "\n",
    "    embedding_sentence = question+ \" \"+word.capitalize()\n",
    "    # embedding_sentence = word\n",
    "    keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
    "    distractor_embeddings = sentencemodel.encode(distractors_new)\n",
    "\n",
    "    # filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors,4,0.7)\n",
    "    max_keywords = min(len(distractors_new),5)\n",
    "    filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors_new,max_keywords,lambdaval)\n",
    "    # filtered_keywords = filtered_keywords[1:]\n",
    "    distractor_list__s2v = [word.capitalize()]\n",
    "#     for wrd in filtered_keywords:\n",
    "#         if wrd.lower() !=word.lower():\n",
    "#             distractor_list_s2v.append(wrd.capitalize())\n",
    "#     distractor_list_s2v = distractor_list2[1:]\n",
    "    \n",
    "    return distractor_list_s2v\n",
    "\n",
    "#__________________________________________________________________________________________________\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple Text = For Conceptnet and Wordnet Example since they do not use the whole text for finding the best fitting synoyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors Conceptnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncommand if you use whole text\n",
    "# key_distractor_list_cn = {}\n",
    "# distractors_cn =[]\n",
    "\n",
    "\n",
    "\n",
    "# #print( keyword_sentence_mapping)\n",
    "# #conceptnet\n",
    "# for keyword in keyword_sentence_mapping:\n",
    "#     print(keyword)\n",
    "    \n",
    "#     #print(\"keyword_sentence_mapping[keyword][0]\",keyword_sentence_mapping[keyword][0])\n",
    "#     #print(\"keyword_sentence_mapping[keyword]\",keyword_sentence_mapping[keyword])\n",
    "#     wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword) \n",
    "#     #print(wordsense)\n",
    "#     if wordsense:\n",
    "#         if len(distractors_cn) ==0:\n",
    "#             distractors_cn = get_distractors_conceptnet(keyword.capitalize())\n",
    "#         if len(distractors_cn) != 0:\n",
    "#             key_distractor_list_cn[keyword] = distractors_cn\n",
    "# print(\"CONCEPTNET: \", key_distractor_list_cn)\n",
    "# print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['domestic cat', 'stocker', 'feeder', 'stray', 'head', 'A dog', 'bitch', 'wild dog', 'wolf', 'jackal', 'hyena', 'fang', 'fox', 'sardinian dhole', 'a dog', 'A mouse', 'A bear', 'A rabbit', 'whale', 'female mammal', 'An armadillo', 'prototherian', 'The beaver', 'metatherian']\n"
     ]
    }
   ],
   "source": [
    "simple_keyword = \"Dog\"\n",
    "language = \"EN\"\n",
    "distractors_cn = get_distractors_conceptnet(simple_keyword.capitalize())\n",
    "print(distractors_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato', 'boga', 'canguro', 'yacaré', 'anguila', 'coyote', 'mosca', 'avión', 'pata']\n"
     ]
    }
   ],
   "source": [
    "simple_keyword = \"Perro\"\n",
    "language = \"ES\"\n",
    "distractors_cn = get_distractors_conceptnet(simple_keyword.capitalize())\n",
    "print(distractors_cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors Wordnet\n",
    "Not sure if it can be adapted to spanish. It doesn't work with German. So here is an English example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_distractor_list_wn = {}\n",
    "\n",
    "# #wordnet\n",
    "# distractors_wn=[]\n",
    "# for keyword in keyword_sentence_mapping: \n",
    "#     wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword) \n",
    "#     if wordsense:\n",
    "#         if len(distractors_wn) ==0:\n",
    "#             distractors_wn = get_distractors_wordnet(keyword.capitalize())\n",
    "#         if len(distractors_wn) != 0:\n",
    "#             key_distractor_list_wn[keyword] = distractors_wn\n",
    "# print(\"WORDNET: \", key_distractor_list_wn)\n",
    "# print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym bitch\n",
      "Synonym dog\n",
      "Synonym fox\n",
      "Synonym hyena\n",
      "Synonym jackal\n",
      "Synonym wild_dog\n",
      "Synonym wolf\n"
     ]
    }
   ],
   "source": [
    "simple_keyword = \"Dog\" \n",
    "distractors_cwv = get_distractors_wordnet(simple_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordnet distractors not found\n"
     ]
    }
   ],
   "source": [
    "simple_keyword = \"Perro\" \n",
    "distractors_cwv = get_distractors_wordnet(simple_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors Sense2Vec\n",
    "Consideres Input text for finding the best Distractors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "nGjjioMpAny3",
    "outputId": "5e3b06ac-1acb-496f-ddfa-c2129c975097"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't read file: cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-278855522e4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mS2V_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;31m#Path to vector folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0ms2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSense2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS2V_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeyword_sentence_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/venv/lib/python3.7/site-packages/sense2vec/sense2vec.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mcache_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"cache\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfreqs_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/venv/lib/python3.7/site-packages/srsly/_json_api.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mujson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mujson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/venv/lib/python3.7/site-packages/srsly/util.py\u001b[0m in \u001b[0;36mforce_path\u001b[0;34m(location, require_exists)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_exists\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't read file: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't read file: cfg"
     ]
    }
   ],
   "source": [
    "#Self-implemented way. Feel free to check out the reference fot other approaches: https://pypi.org/project/sense2vec/\n",
    "#you can download the reddit vectors there\n",
    "\n",
    "from sense2vec import Sense2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "S2V_PATH = \"\"#Path to vector folder\n",
    "s2v = Sense2Vec().from_disk(S2V_PATH)\n",
    "\n",
    "for keyword in keyword_sentence_mapping:   \n",
    "    sentence = keyword_sentence_mapping[keyword][0]    \n",
    "    sentence_transformer_model = SentenceTransformer('bert-base-german-cased') #choose needed model\n",
    "    key_distractor_list_s2v=get_distractors_s2v(keyword, sentence,s2v,sentence_transformer_model,40,0.2)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BAC_Question Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
